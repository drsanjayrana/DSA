{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net based Image Segmentation for locating Radion Frequencey Interference Patches on Sentinel-1 images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:14.705937Z",
     "start_time": "2018-10-14T07:38:14.690314Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:21.173041Z",
     "start_time": "2018-10-14T07:38:14.705937Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initliasations\n",
    "\n",
    "# Keras deep learning library\n",
    "import keras\n",
    "from keras.models import load_model, Model\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, concatenate, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:21.527056Z",
     "start_time": "2018-10-14T07:38:21.173041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Geospatial libraries to handle georeferenced raster and vector data\n",
    "import fiona\n",
    "from fiona import collection\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, mapping, shape\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio import windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.447929Z",
     "start_time": "2018-10-14T07:38:21.527056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib for data plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Scikit learn and scikit image libraries for machine learning and computer vision tools.\n",
    "from skimage import io, exposure, measure\n",
    "from sklearn.metrics import jaccard_similarity_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.481623Z",
     "start_time": "2018-10-14T07:38:23.447929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful python libaries for manipulating data\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Pandas to bring dataframe structure\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy for numerical processing and arrays\n",
    "import numpy as np\n",
    "\n",
    "# Initializse a random number generator to ensure results are reproducible\n",
    "np.random.seed(7)\n",
    "# turn warnig off\n",
    "np.warnings.filterwarnings('ignore')\n",
    "# print floating points as using fixed point notation i.e. not scientific\n",
    "np.set_printoptions(suppress=True)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.493888Z",
     "start_time": "2018-10-14T07:38:23.481623Z"
    }
   },
   "outputs": [],
   "source": [
    "# set some useful paths\n",
    "# RAW INPUTS\n",
    "#root = 'i:/DSA/Detect_RFI/'\n",
    "root = '/Volumes/NewVolume/DSA/Detect_RFI/'\n",
    "s1_train_dir = os.path.join(root, 'DATA/S1_TRAIN/SPLIT')  # full res input S1 ARD for training\n",
    "s1_test_dir = os.path.join(root, 'DATA/S1_TEST/SPLIT')  # full res input S1 ARD for testing\n",
    "\n",
    "# perhaps would be an idea to transform ARD into some generic metric surface like normalised (scale-min max)slope, curvature\n",
    "# so that model works for images from anywhere\n",
    "labels_dir = os.path.join(root, 'DATA/LABELS')  # mask polygons\n",
    "\n",
    "# DERIVED INPUTS\n",
    "image_chips_dir = os.path.join(root, 'DATA/IMAGE_CHIPS')  # pre-procssed image tiles and mask root folder\n",
    "model_dir = os.path.join(root, 'MODELS')  # contains the machine learning models\n",
    "predictions_dir = os.path.join(root, 'PREDICTIONS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the masks and store them in a dictionary indexed by the Sentinel-1 tile name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.517892Z",
     "start_time": "2018-10-14T07:38:23.501892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training data polygons from masks shapefile\n",
    "# open the masks polygons and create an array of geometries for unique tilename\n",
    "def makeMasks(shapeFileName,labelFieldName):\n",
    "    LU_LABELS = {}\n",
    "    with fiona.open(shapeFileName, 'r') as landcovers:\n",
    "        for feature in landcovers:\n",
    "            LU_LABEL = feature['properties'][labelFieldName]\n",
    "            mask = feature[\"geometry\"]\n",
    "            if LU_LABEL in LU_LABELS:\n",
    "                oldArray = LU_LABELS[LU_LABEL]\n",
    "                oldArray.append(mask)\n",
    "            else:\n",
    "                LU_LABELS[LU_LABEL] = [mask]\n",
    "    return LU_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.949897Z",
     "start_time": "2018-10-14T07:38:23.521892Z"
    }
   },
   "outputs": [],
   "source": [
    "# read land cover polygon  into geometry arrays that rasterio will use to create masks\n",
    "labels_shp = os.path.join(labels_dir, 'RFI_Masks_Final.shp')\n",
    "LU_LABELS = makeMasks(labels_shp,'TILENAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Input Data i.e. Prepare Image Chips and Mask the non-data/data areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.962761Z",
     "start_time": "2018-10-14T07:38:23.949897Z"
    }
   },
   "outputs": [],
   "source": [
    "def maskImage(input_image_name,allPolygons):\n",
    "    try:\n",
    "        temp_file_name = input_image_name.replace(\".tif\",\"_temp.tif\")\n",
    "        maskDone = False\n",
    "        with rasterio.open(input_image_name) as src:\n",
    "            mask_image, mask_transform = rasterio.mask.mask(src,allPolygons,nodata=0)\n",
    "            mask_meta = src.meta.copy()                   \n",
    "            # skip this tile if it only has nodata otherwise\n",
    "            # we are overfeeding nodata\n",
    "            uniqueLabels = np.unique(mask_image[0,:,:])\n",
    "            if len(uniqueLabels) < 2 and uniqueLabels[0] ==0:                \n",
    "                maskDone= input_image_name\n",
    "            else:\n",
    "                with rasterio.open(temp_file_name, 'w', **mask_meta) as dest:\n",
    "                    dest.write(mask_image)\n",
    "                    maskDone = temp_file_name\n",
    "        \n",
    "        if (maskDone == input_image_name):\n",
    "            os.remove(input_image_name)\n",
    "        elif (maskDone == temp_file_name):\n",
    "            os.remove(input_image_name)\n",
    "            os.rename(temp_file_name,input_image_name)\n",
    "        return True    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return maskDone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:23.978764Z",
     "start_time": "2018-10-14T07:38:23.962761Z"
    }
   },
   "outputs": [],
   "source": [
    "# function by cate to create 3D tiles \n",
    "def get_tile_images(arr, newshape):\n",
    "    oldshape = np.array(arr.shape)\n",
    "    repeats = (oldshape / newshape).astype(int)\n",
    "    tmpshape = np.column_stack([repeats, newshape]).ravel()\n",
    "    order = np.arange(len(tmpshape))\n",
    "    order = np.concatenate([order[::2], order[1::2]])\n",
    "    #pdb.set_trace()\n",
    "    # newshape must divide oldshape evenly or else ValueError will be raised\n",
    "    return arr.reshape(tmpshape).transpose(order).reshape(-1, *newshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:24.003293Z",
     "start_time": "2018-10-14T07:38:23.978764Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the s1 ard image, apply the standard deviation scaling\n",
    "# it to the ARD\n",
    "def scaleImage(S1_ImageName, tileYSize, tileXSize):\n",
    "    with rasterio.open(S1_ImageName) as S1_src:\n",
    "        newHeight = int(S1_src.height / tileYSize) * tileYSize\n",
    "        newWidth = int(S1_src.width / tileXSize) * tileXSize\n",
    "        if (newHeight < tileYSize or newWidth < tileXSize):\n",
    "            print(\"newhieght or newwidth are smaller than tile sizes\")\n",
    "            return None,None,None,None\n",
    "        \n",
    "        if (newHeight < newWidth):\n",
    "            newWidth = newHeight\n",
    "        elif (newWidth < newHeight):\n",
    "            newHeight = newWidth\n",
    "    \n",
    "        S1_image = S1_src.read(window=((0, newWidth), (0, newHeight)))\n",
    "        \n",
    "#        if (np.isnan(S1_image.max()) or np.isnan(S1_image.min())):\n",
    "#            print(\"nan found\")\n",
    "#            return None,None,None,None\n",
    "#        if (S1_image.max()<0.0000001 and S1_image.min()<0.0000001):\n",
    "#            print(\"near zero found\")\n",
    "#            return None,None,None,None\n",
    "     \n",
    "#        print(str(S1_image.max()))\n",
    "#        print(str(S1_image.min()))\n",
    "#        print (S1_image.shape)\n",
    "#        S1_image = exposure.equalize_hist(S1_image)        \n",
    "        # StandardScaler(copy=True, with_mean=False, with_std=True).fit_transform(S1_image)        \n",
    "        # show(S1_image)\n",
    "        \n",
    "        crs = S1_src.crs\n",
    "        out_metadata = S1_src.meta.copy()\n",
    "        out_transform = S1_src.transform   \n",
    "        \n",
    "    return S1_image, out_metadata, out_transform, crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:41:47.805944Z",
     "start_time": "2018-10-14T07:41:47.774690Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepImages(s1_images_dir,image_chips_dir,allPolygons):\n",
    "    # read the s1 ARD image names that we have and store them in a dictionary\n",
    "    \n",
    "    for filename in os.listdir(s1_images_dir):\n",
    "        if filename.lower().endswith(\".tif\"):        \n",
    "            # iterate through the S1 ARD images\n",
    "            # create image chips\n",
    "            \n",
    "            tileName = filename[:4] \n",
    "            if tileName in allPolygons:            \n",
    "                RFI_Masks = allPolygons[tileName]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            s1_image = os.path.join(s1_images_dir,filename)   \n",
    "            print(\"Processing: \"+s1_image)\n",
    "            \n",
    "            scaled_image, inputMetadata, inputTransform, crs = scaleImage(s1_image,256,256) #1\n",
    "            if (scaled_image is None):\n",
    "                continue\n",
    "            \n",
    "            ULX = inputTransform.c\n",
    "            ULY = inputTransform.f\n",
    "            cellSizeX = inputTransform.a\n",
    "            cellSizeY = inputTransform.e  \n",
    "            \n",
    "            scaled_image_tiled = get_tile_images(scaled_image,(1,256,256)) #2\n",
    "            \n",
    "            # write image chips\n",
    "            numtiles,numbands,rows,cols = scaled_image_tiled.shape\n",
    "            rowOffset = 0\n",
    "            blockSize = int(math.sqrt(numtiles))\n",
    "            for tileIndexAlongY in range(0,blockSize):\n",
    "                tileULY = ULY + (tileIndexAlongY * cellSizeY * 256)\n",
    "                for tileIndexAlongX in range(0,blockSize):\n",
    "                    tileULX = ULX + (tileIndexAlongX * cellSizeX * 256)\n",
    "                    # print(str(tileULX)+\",\"+str(tileULY))\n",
    "                    globalTileIndex = tileIndexAlongY + tileIndexAlongX + rowOffset\n",
    "                    aTileImage = scaled_image_tiled[globalTileIndex,:,:]\n",
    "                           \n",
    "                    # aTileImage = np.expand_dims(aTileImage,0)\n",
    "                    tileMetadata = inputMetadata.copy()\n",
    "                    tileTransform = Affine(cellSizeX,inputTransform.b,tileULX,inputTransform.d,cellSizeY,tileULY)                \n",
    "\n",
    "                    # two more bands have been added                \n",
    "                    tileMetadata.update({'driver': 'GTiff',\n",
    "                                         'dtype': 'float32',\n",
    "                                         'count': 1,\n",
    "                                         'height': 256,\n",
    "                                         'width': 256,\n",
    "                                         'crs': crs,\n",
    "                                         'transform': tileTransform,\n",
    "                                         'nodata':0})              \n",
    "                    tileFileName = os.path.basename(s1_image).replace(\".tif\",\"_\"+str(globalTileIndex).zfill(5)+\".tif\")\n",
    "                    tileFullName = os.path.join(image_chips_dir, tileFileName)\n",
    "                    with rasterio.open(tileFullName, 'w', **tileMetadata) as dest:\n",
    "                        dest.write(aTileImage)\n",
    "                \n",
    "                    #now mask nodata areas in the image\n",
    "                    maskDone = maskImage(tileFullName,RFI_Masks)\n",
    "                    if (not maskDone):\n",
    "                        print(\"failed\")\n",
    "                    \n",
    "                rowOffset = rowOffset + (blockSize-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:11:01.353210Z",
     "start_time": "2018-10-14T09:11:01.321966Z"
    }
   },
   "outputs": [],
   "source": [
    "def maskImages(images_dir,labels_chips_dir,LU_LABELS):\n",
    "    for filename in os.listdir(images_dir):\n",
    "        if filename.endswith(\".tif\"):\n",
    "            image_path = os.path.join(images_dir,filename)\n",
    "            #find the image tile name\n",
    "            tileName = filename[:4]\n",
    "            #create a list of output ndarrays\n",
    "            mask_array = []\n",
    "            #iterate through the mask vectors\n",
    "            with rasterio.open(image_path) as src:           \n",
    "                masks = LU_LABELS[tileName]\n",
    "                mask_image, mask_transform = rasterio.mask.mask(src,masks,nodata=0)\n",
    "                mask_meta = src.meta.copy()\n",
    "                mask_image[mask_image != 0] = 1\n",
    "\n",
    "                mask_image = mask_image[0].astype('uint8')\n",
    "                mask_image = np.expand_dims(mask_image, 0)\n",
    "            \n",
    "                #save the final mask image\n",
    "                mask_meta.update({'driver': 'GTiff',\n",
    "                             'dtype': 'uint8',\n",
    "                             'count': 1,\n",
    "                             'height': mask_image.shape[1],\n",
    "                             'width': mask_image.shape[2],\n",
    "                             'crs': src.crs,\n",
    "                             'transform': mask_transform})\n",
    "                label_file_name = os.path.join(labels_chips_dir,filename.replace(\".tif\",\"_labels.tif\"))\n",
    "                with rasterio.open(label_file_name, 'w', **mask_meta) as dest:\n",
    "                    dest.write(mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network\n",
    "## The last approach will implement Unet - a popular convolutional neural network for image classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T07:38:24.072099Z",
     "start_time": "2018-10-14T07:38:24.048603Z"
    }
   },
   "outputs": [],
   "source": [
    "###Set up the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T08:49:49.085505Z",
     "start_time": "2018-10-14T07:42:30.272472Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess the training RGB and CIR images to resample CIR, extract NIR band, append NIR band to RGB, split into small tiles\n",
    "train_chips_dir = os.path.join(image_chips_dir, 'TRAIN')\n",
    "prepImages(s1_train_dir,train_chips_dir, LU_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T08:52:02.033018Z",
     "start_time": "2018-10-14T08:52:02.017395Z"
    }
   },
   "outputs": [],
   "source": [
    "# using rasterio mask function to mask pixels using LU_LABEL in the training images\n",
    "train_images_dir = os.path.join(image_chips_dir,'TRAIN')\n",
    "train_labels_chips_dir = os.path.join(train_images_dir,'LABELS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:11:12.469017Z",
     "start_time": "2018-10-14T09:11:09.376228Z"
    }
   },
   "outputs": [],
   "source": [
    "maskImages(train_images_dir,train_labels_chips_dir,LU_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:11:16.865365Z",
     "start_time": "2018-10-14T09:11:16.849736Z"
    }
   },
   "outputs": [],
   "source": [
    "# set paths to the training data\n",
    "image_chips = train_images_dir\n",
    "label_chips = train_labels_chips_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:31:44.311995Z",
     "start_time": "2018-10-14T09:31:44.274211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class to store data and respective labels\n",
    "# imagese are split into halves..first half used for training, and remaining half into validation\n",
    "class train_data():\n",
    "    \n",
    "    def __init__(self, image, label):\n",
    "        self.image = []\n",
    "        self.label = []\n",
    "        for file in os.listdir(image):\n",
    "            if file.endswith(\".tif\"):\n",
    "                label_file= os.path.join(label,file.replace(\".tif\",\"_labels.tif\"))\n",
    "                imgArray = io.imread(image+\"/\"+file,0)\n",
    "                #print(imgArray.shape)\n",
    "                imgArray = np.expand_dims(imgArray,2)\n",
    "                #print(imgArray.shape)\n",
    "                self.image.append(imgArray)\n",
    "                lblArray = io.imread(label_file,0)\n",
    "                lblArray = np.expand_dims(lblArray,2)\n",
    "                self.label.append(lblArray)\n",
    "    \n",
    "    # training half\n",
    "    def get_image(self):\n",
    "        return np.array(self.image[:int(len(self.image)/2)])\n",
    "\n",
    "    def get_label(self):\n",
    "        return np.array(self.label[:int(len(self.image)/2)])\n",
    "    \n",
    "    # validation half\n",
    "    def get_validation_image(self):\n",
    "        return np.array(self.image[int(len(self.image)/2):])\n",
    "    \n",
    "    def get_validation_label(self):\n",
    "        return np.array(self.label[int(len(self.image)/2):])\n",
    "        \n",
    "    def set_image(self, new_images):\n",
    "        self.image = new_image\n",
    "    \n",
    "    def set_label(self,new_label):\n",
    "        self.label = new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:31:47.205648Z",
     "start_time": "2018-10-14T09:31:46.854299Z"
    }
   },
   "outputs": [],
   "source": [
    "# run the training data creation\n",
    "train_set = train_data(image_chips, label_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:31:54.313313Z",
     "start_time": "2018-10-14T09:31:54.087940Z"
    }
   },
   "outputs": [],
   "source": [
    "# access the training set\n",
    "train_images = train_set.get_image()\n",
    "train_label = train_set.get_label()\n",
    "# access the validation set\n",
    "validation_image = train_set.get_validation_image()\n",
    "validation_label = train_set.get_validation_label()\n",
    "# one hot encode the labels\n",
    "train_label_encoded = to_categorical(train_label, num_classes=2)\n",
    "validation_label_encoded = to_categorical(validation_label, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:31:57.703080Z",
     "start_time": "2018-10-14T09:31:57.671834Z"
    }
   },
   "outputs": [],
   "source": [
    "show(train_images[44,:,:,:])\n",
    "show(train_label[44,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:32:06.481315Z",
     "start_time": "2018-10-14T09:32:06.459162Z"
    }
   },
   "outputs": [],
   "source": [
    "# check that Keras expects the bands to be passed last - i.e. the data is shaped (256, 256, 12)\n",
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_zhixuhao(pretrained_weights = None,input_size = (256,256,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(2, (1, 1), activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-14T09:32:11.492124Z",
     "start_time": "2018-10-14T09:32:11.459838Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the Unet architecture\n",
    "def unet_cate(pretrained_weights = None,input_size = (256,256,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    # use softmax in order to only output one class per pixel (unlike sigmoid which can have multiclass)\n",
    "    conv10 = Conv2D(2, (1, 1), activation='softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "        \n",
    "    #model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer=Adam(lr=1e-9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # see if a different loss and metrics can improve e.g. sorenson-dice\n",
    "    # model.compile(optimizer=Adam(lr=1e-9), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-14T09:32:14.251Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = unet_cate()\n",
    "model.fit(train_images, train_label_encoded, validation_data=(validation_image, validation_label_encoded), epochs=100,\n",
    "          batch_size=8, shuffle=True)\n",
    "# save the model and weights\n",
    "model.save(os.path.join(model_dir, '20181014_Unet_100epoch_RFI_dice.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(os.path.join(model_dir, '20180905_Unet_10epoch.h5'))\n",
    "\n",
    "# lower the learning rate\n",
    "#lr = 0.0001\n",
    "#K.set_value(model.optimizer.lr, lr)\n",
    "\n",
    "#model.fit(train_images, train_label_encoded, validation_data=(validation_image, validation_label_encoded), epochs=10,\n",
    "#          batch_size=10, shuffle=True)\n",
    "#model.save(os.path.join(model_dir, '20180905_Unet_20epoch.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(os.path.join(model_dir, '20180905_Unet_20epoch.h5'))\n",
    "\n",
    "# try increasing the learning rate as little improvement was seen in previous epochs\n",
    "#lr = 0.001\n",
    "#K.set_value(model.optimizer.lr, lr)\n",
    "\n",
    "#model.fit(train_images, train_label_encoded, validation_data=(validation_image, validation_label_encoded), epochs=10,\n",
    "#          batch_size=8, shuffle=True)\n",
    "#model.save(os.path.join(model_dir, '20180905_Unet_30epoch.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the accuracy of the predictions using known test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test RGB and CIR images to resample CIR, extract NIR band, append NIR band to RGB, split into small tiles\n",
    "test_chips_dir = os.path.join(image_chips_dir, 'TEST')\n",
    "prepImages(s1_test_dir,test_chips_dir,LU_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = os.path.join(image_chips_dir,'TEST')\n",
    "test_labels_chips_dir = os.path.join(test_images_dir,'LABELS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using rasterio mask function to mask pixels using LU_LABEL in the test images\n",
    "maskImages(test_images_dir,test_labels_chips_dir,LU_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the trained model\n",
    "unet_model = load_model(os.path.join(model_dir, '20181014_Unet_100epoch_RFI_dice.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_image_chips = test_images_dir \n",
    "test_label_chips = test_labels_chips_dir \n",
    "\n",
    "class test_data():\n",
    "    \n",
    "    def __init__(self, image, label):\n",
    "        self.image = []\n",
    "        self.label = []\n",
    "        self.filename = [] # store the filename because we can use it later to recreate the predicted image for it\n",
    "        \n",
    "        for file in os.listdir(image):\n",
    "            \n",
    "            if file.endswith(\".tif\"):\n",
    "                label_file= os.path.join(label,file.replace(\".tif\",\"_labels.tif\"))\n",
    "                imgArray = io.imread(image+\"/\"+file,0)\n",
    "                imgArray = np.expand_dims(imgArray, 2) # this is done to meet keras channel order needs\n",
    "                self.image.append(imgArray)\n",
    "                lblArray = io.imread(label_file,0)\n",
    "                lblArray = np.expand_dims(lblArray,2)\n",
    "                self.label.append(lblArray)\n",
    "                self.filename.append(file)\n",
    "               \n",
    "    def get_image(self):\n",
    "        return np.array(self.image[:int(len(self.image))])\n",
    "\n",
    "    def get_label(self):\n",
    "        return np.array(self.label[:int(len(self.image))])\n",
    "        \n",
    "    def set_image(self, new_images):\n",
    "        self.image = new_image\n",
    "    \n",
    "    def set_label(self,new_label):\n",
    "        self.label = new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test set\n",
    "test_set = test_data(test_image_chips, test_label_chips)\n",
    "# access the test images and labels\n",
    "test_images = test_set.get_image()\n",
    "test_label = test_set.get_label()\n",
    "# one-hot-encode the labels\n",
    "test_label_encoded = to_categorical(test_label, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set using the trained model\n",
    "test_predict = unet_model.predict(test_images, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the predictions\n",
    "test_predict_decoded = np.argmax(test_predict, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# to calculate all metrics\n",
    "def accuracy_statistics(true_class, predicted_class):\n",
    "   \n",
    "    # classification report\n",
    "    #target_names = [\"Class {}\".format(i) for i in range(3)]\n",
    "    print('Classification Report')\n",
    "    print(classification_report(true_class, predicted_class))\n",
    "   \n",
    "    # jaccard\n",
    "    print('Jaccard Score:')\n",
    "    print(jaccard_similarity_score(true_class, predicted_class))\n",
    "   \n",
    "    # confusion matrix\n",
    "    # plt.figure()\n",
    "    print ('Confusion Matrix')\n",
    "    #print(confusion_matrix(true_class, predicted_class))\n",
    "    cnf_matrix = confusion_matrix(true_class, predicted_class)\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['NoData', 'RFI Area'],\n",
    "                          title='Confusion Matrix')\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['NoData', 'RFI Area'], normalize=True,\n",
    "                          title='Normalized Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate accuracy statistics\n",
    "accuracy_statistics(test_label.ravel(), test_predict_decoded.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the test_images, find the related predicted arrays, find the geotransformation of the test_image it came from\n",
    "# and the save the predicted array as an image using the geotransform\n",
    "for tileNum in range(len(test_predict_decoded)):\n",
    "    imgFileName = test_set.filename[tileNum]\n",
    "    imageFullName = os.path.join(test_image_chips,imgFileName)\n",
    "    with rasterio.open(imageFullName) as src:\n",
    "        out_transform = src.transform\n",
    "        out_meta = {'driver': 'GTiff',\n",
    "                    'dtype': 'uint8',\n",
    "                    'count': 1,\n",
    "                    'height': 256,\n",
    "                    'width': 256,\n",
    "                    'crs': src.crs,\n",
    "                    'transform': out_transform,\n",
    "                    'nodata':0}\n",
    "        predictionFileName = os.path.join(predictions_dir, 'UNET',imgFileName.replace(\".tif\",\"_prediction.tif\"))\n",
    "        with rasterio.open(predictionFileName, 'w', **out_meta) as dest:\n",
    "                    im_prediction = test_predict_decoded[tileNum,:,:]\n",
    "                    im_prediction = np.expand_dims(im_prediction, 0)\n",
    "                    im_prediction = im_prediction.astype('uint8')\n",
    "                    dest.write(im_prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
